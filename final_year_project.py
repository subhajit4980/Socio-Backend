# -*- coding: utf-8 -*-
"""Final_Year_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DGsDnbsKf9NG6TGTXlL5_3ddT3OqHqo_

# **Machine Learning Based Classification of Suicidal Thoughts From Social Media Comments**

### **Data Preprocessing**
"""

#Importing Libraries

import numpy as np
import pandas as pd
import timeit

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import metrics
from tensorflow.keras.models import Sequential

# Get the data

full_data_path = "/content/train.csv"
test_data_X_path = "/content/test.csv"
test_data_y_path = "/content/test_labels.csv"

full_data = pd.read_csv(full_data_path)
test_data_X = pd.read_csv(test_data_X_path)
test_data_y = pd.read_csv(test_data_y_path)

full_data.head()

print(f"There are {len(full_data)} observations in full data.")

test_data_X.head()

test_data_y.head()

full_data.columns

full_data["comment_text"].values

full_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values

train_sentences=full_data['comment_text']
test_sentences=test_data_X['comment_text']

import re
import string

# Function to clean text by normalizing it and removing contractions
def clean_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Replace contractions with their expanded forms
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r"\'scuse", " excuse ", text)
    # Remove all non-alphabetic characters
    text = re.sub("[^a-z]", " ", text)
    # Remove all non-word characters
    text = re.sub('\W', ' ', text)
    # Replace multiple spaces with a single space
    text = re.sub('\s+', ' ', text)
    # Strip leading and trailing whitespace
    text = text.strip(' ')
    return text

# Function to remove punctuation from text
def remove_punctuation(text):
    return text.translate(str.maketrans("", "", string.punctuation))

# Function to remove special characters like newlines and tabs
def remove_special_chars(text):
    return text.replace("\n", " ").replace("\t", " ").strip()

# Function to normalize spacing by collapsing multiple spaces into a single space
def normalize_spacing(text):
    return " ".join(text.split())


# Main text processing function that applies all preprocessing steps
def process_text(text):
    text= clean_text(text)
    text = remove_punctuation(text)
    text = remove_special_chars(text)
    text = normalize_spacing(text)
    return text


# apply preprocessing
train_sentences = train_sentences.map(process_text)
test_sentences = test_sentences.map(process_text)

print(train_sentences)

print(test_sentences)

"""### **Natural Language Processing (NLP)**"""

#Import Libraries

import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

nltk.download('words')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

words = set(nltk.corpus.words.words())
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Tokenization
def tokenization_process(text):
    return word_tokenize(text)

# Apply preprocessing
train_sentences = train_sentences.map(tokenization_process)
test_sentences = test_sentences.map(tokenization_process)

print(train_sentences)

print(test_sentences)

# Remove stopwords
def remove_stopwords(text):
    tokens = [word for word in text if word not in stop_words]
    return tokens

# Apply preprocessing
train_sentences = train_sentences.map(remove_stopwords)
test_sentences = test_sentences.map(remove_stopwords)

print(train_sentences)

print(test_sentences)

#Lemmatization
def lemmatize_words(text):
    tokens = [lemmatizer.lemmatize(word) for word in text]
    return ' '.join(tokens)

# Apply preprocessing
train_sentences = train_sentences.map(lemmatize_words)
test_sentences = test_sentences.map(lemmatize_words)

print(train_sentences)

print(test_sentences)

full_data['comment_text']=train_sentences
test_data_X['comment_text']=test_sentences

full_data.head()

test_data_X.head()

# Merging the X and y part together
test_dataframe = pd.merge(test_data_X, test_data_y, how="inner", on="id")

# Remove all the rows having missing values (-1)
test_data = test_dataframe[test_dataframe["toxic"] != -1].reset_index(drop=True)

test_data.sample(5)

print(f"We have {len(test_dataframe)} observations in test data.")

# Remove the unnecessary data

del test_data_X
del test_data_y

full_data.columns

full_data["comment_text"].values

full_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values

from sklearn.model_selection import train_test_split

# Split the data into training set and validation set
train_data, val_data = train_test_split(full_data, train_size=0.8, random_state=10)

train_data

val_data

train_sentences=train_data['comment_text']
train_labels = train_data.iloc[:, 2:]

val_sentences=val_data['comment_text']
val_labels = val_data.iloc[:, 2:]

test_sentences=test_data['comment_text']
test_labels = test_data.iloc[:, 2:]

train_sentences

train_labels

val_sentences

val_labels

test_sentences

test_labels

# Convert the pandas DataFrame or Series to a NumPy array

train_sentences = train_sentences.to_numpy()
train_labels = train_labels.to_numpy()
val_sentences = val_sentences.to_numpy()
val_labels = val_labels.to_numpy()
test_sentences =test_sentences.to_numpy()
test_labels=test_labels.to_numpy()

train_sentences

train_labels

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding

# Define the maximum number of tokens (words) to be considered in the tokenizer
max_tokens=100000

# Create a Tokenizer object that will be used to vectorize the text corpus
tokenizer = Tokenizer(num_words=max_tokens)
# Fit the tokenizer on the training sentences to build the word index
tokenizer.fit_on_texts(train_sentences)

# convert sentences to integer sequences
# Each word in a sentence is replaced with its corresponding integer index
train_sequences = tokenizer.texts_to_sequences(train_sentences)
val_sequences = tokenizer.texts_to_sequences(val_sentences)
test_sequences=tokenizer.texts_to_sequences(test_sentences)

train_sequences

train_sequences[0]

val_sequences[0]

test_sequences[0]

# ----------------------This is taken from Kaggle------------------------------

max_length = 300 #depending upon the mean length of the comments
padding_type = "post" # Add padding at the end of the sequence
trunc_type = "post"  # Truncate sequences at the end if they exceed max_length

# Apply padding and truncating to sequences
train_sequences = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
val_sequences = pad_sequences(val_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
test_sequences=  pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

# Remove the unnecessary data

del full_data
del test_dataframe

"""### **Model Building**"""

#Long Short-Term Memory(LSTM) Model

# Define the Sequential model
model = Sequential([
    layers.Embedding(max_tokens+1, 32),  # Embedding layer: Converts input tokens (indices) into dense vectors of fixed size
    layers.Bidirectional(layers.LSTM(32, return_sequences=False)), # return_sequences=False: Only returns the output of the last time step
    layers.Dense(256, activation="relu"), # Dense (Fully connected) layer with ReLU activation: Learns complex patterns in the data
    layers.Dense(256, activation="relu"),
    layers.Dense(128, activation="relu"),
    layers.Dense(6, activation="sigmoid")  # Output Dense layer with sigmoid activation: Outputs probabilities for multi-label classification
])


# Compile the model with specified settings
model.compile(
    optimizer="adam", # Adam is an adaptive learning rate optimization algorithm that has been popular in deep learning
    loss="binary_crossentropy", # Binary cross-entropy is a common loss function for binary classification tasks
    metrics=metrics.BinaryAccuracy() # BinaryAccuracy computes the frequency with which predictions match labels for binary classification
)


callbacks = [
    # EarlyStopping callback to stop training when the validation loss has stopped improving.
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=5,
        restore_best_weights=True, # Restore the model weights from the epoch with the best validation loss.
        start_from_epoch=1, # Start monitoring from epoch 1 to allow the model to train for at least one epoch.
        verbose=1 # Verbosity mode, 1 = progress messages are printed.
    ),
    # ReduceLROnPlateau callback to reduce the learning rate when the validation loss has stopped improving.
    keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5, # Factor by which the learning rate will be reduced. new_lr = lr * factor.
        patience=3,
        verbose=1, # Verbosity mode, 1 = progress messages are printed.
        min_lr=1e-6  # Lower bound on the learning rate.
    )
]

batch_size=64 # Number of samples per gradient update

start = timeit.default_timer() # Record the start time of model training

# Fit the model to the training data
model_history = model.fit(
    train_sequences,
    train_labels,
    epochs=5, # Number of epochs (iterations over the entire dataset)
    verbose=1, # Verbosity mode (1 for progress bar, 0 for silent)
    callbacks=callbacks, # List of callbacks to apply during training
    batch_size=batch_size, # Number of samples per gradient update
    validation_data=(val_sequences, val_labels),
).history # History object containing training metrics

end = timeit.default_timer() # Record the end time of model training

# Print the time taken for model training
print(f"It took {end - start} seconds to train the model.")

"""### **Model Testing**"""

# //----------------------Kaggle--------------------
# Evaluate the accuracy of the model on the test set
test_accuracy = model.evaluate(test_sequences, test_labels)
print("Test Accuracy:", test_accuracy[1])

# --------------------From Kaggle---------------------------
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report
# Calculate the precision, recall, and F1 scores of the model
predictions = model.predict(test_sequences)

# Convert probability values to binary predictions (threshold can be adjusted)
threshold = 0.5
binary_predictions = (predictions > threshold).astype(int)

# Calculate the precision, recall, and F1 scores of each label
precision = precision_score(test_labels, binary_predictions, average='weighted')
recall = recall_score(test_labels, binary_predictions, average='weighted')
f1 = f1_score(test_labels, binary_predictions, average='weighted')

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# Generate classification report
class_report = classification_report(test_labels, binary_predictions)
print(class_report)

"""### **Visualization**"""

from matplotlib import pyplot as plt
plt.figure(figsize=(8,5))
pd.DataFrame(model_history).plot()
plt.show()

model.summary()

from tensorflow.keras.utils import plot_model
plot_model(model=model, show_shapes=True)

"""### **Predictive Model**"""

import numpy as np
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
 # Assuming text_vectorization function is defined elsewhere
# Define stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# --------------------Taken from kaggle------------------

def predict(text):

  # Clean the text
  text=process_text(text)

  # Tokenize the text
  text = word_tokenize(text)

  # Remove stopwords
  text = [word for word in text if word not in stop_words]

  # Lemmatize the words
  text = [lemmatizer.lemmatize(word) for word in text]

  # Join
  text = ' '.join(text)
  sequences = tokenizer.texts_to_sequences([text])

  padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
  # get predictions for toxicity
  predictions = model.predict(padded_sequences)[0]

  # Format prediction text
  prediction_dict = {}
  for i, col in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):
      prediction_dict[col]=predictions[i]

  return prediction_dict

predict("'You freaking suck! I am going to hit you.'")

"""### **Model Save**"""

import json

# 1. Save the tokenizer to a JSON file
tokenizer_json = tokenizer.to_json()
with open('tokenizer.json', 'w', encoding='utf-8') as f:
    f.write(tokenizer_json)

model.save('model.h5')

